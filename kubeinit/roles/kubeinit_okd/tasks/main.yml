---
# Copyright kubeinit contributors
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

- name: Prepare services if needed
  ansible.builtin.include_role:
    name: "../../roles/kubeinit_services"
    tasks_from: prepare_services.yml
    public: true
  vars:
    task_completed: "{{ hostvars['kubeinit-facts'] is defined }}"
  when: not task_completed

- block:
    - name: Check to see if we should stop here
      ansible.builtin.debug: msg="Stopping before '{{ kubeinit_stop_before_task }}'"
    - name: End play
      ansible.builtin.meta: end_play
  when: kubeinit_stop_before_task is defined and kubeinit_stop_before_task == 'task-deploy-cluster'

- name: Deploy the cluster bootstrap node
  ansible.builtin.include_role:
    name: "../../roles/kubeinit_libvirt"
    tasks_from: deploy_coreos_guest.yml
    public: yes
  with_items:
    - "{{ groups['extra_nodes'] | list }}"
  loop_control:
    loop_var: cluster_role_item
  vars:
    kubeinit_deployment_node_name: "{{ cluster_role_item }}"
    kubeinit_deployment_delegate: "{{ hostvars[cluster_role_item].target }}"
    kubeinit_deployment_role: bootstrap

- name: Deploy the cluster master nodes
  ansible.builtin.include_role:
    name: "../../roles/kubeinit_libvirt"
    tasks_from: deploy_coreos_guest.yml
    public: yes
  with_items:
    - "{{ groups['all_controller_nodes'] | list }}"
  loop_control:
    loop_var: cluster_role_item
  vars:
    kubeinit_deployment_node_name: "{{ cluster_role_item }}"
    kubeinit_deployment_delegate: "{{ hostvars[cluster_role_item].target }}"
    kubeinit_deployment_role: master

- name: Verify that controller nodes are ok
  ansible.builtin.shell: |
    set -o pipefail
    export KUBECONFIG=~/install_dir/auth/kubeconfig; \
    oc get nodes | grep master | grep " Ready"
  args:
    executable: /bin/bash
  register: cmd_res_nodes_ok
  changed_when: "cmd_res_nodes_ok.rc == 0"
  retries: 60
  delay: 60
  until: cmd_res_nodes_ok.stdout_lines | default([]) | list | count == groups['all_controller_nodes'] | count
  delegate_to: "{{ kubeinit_provision_service_node }}"

- name: Use single node cluster
  ansible.builtin.shell: |
    set -o pipefail
    export KUBECONFIG=~/install_dir/auth/kubeconfig
    oc get nodes
    oc patch clusterversion/version --type='merge' -p "$(cat <<- EOF
    spec:
      overrides:
        - group: apps/v1
          kind: Deployment
          name: etcd-quorum-guard
          namespace: openshift-machine-config-operator
          unmanaged: true
    EOF
    )"
    oc scale --replicas=1 deployment/etcd-quorum-guard -n openshift-machine-config-operator || true
    oc scale --replicas=1 ingresscontroller/default -n openshift-ingress-operator || true
    oc scale --replicas=1 deployment.apps/console -n openshift-console || true
    oc scale --replicas=1 deployment.apps/downloads -n openshift-console || true
    oc scale --replicas=1 deployment.apps/oauth-openshift -n openshift-authentication || true
    oc scale --replicas=1 deployment.apps/packageserver -n openshift-operator-lifecycle-manager || true
    # Optional
    oc scale --replicas=1 deployment.apps/prometheus-adapter -n openshift-monitoring || true
    oc scale --replicas=1 deployment.apps/thanos-querier -n openshift-monitoring || true
    oc scale --replicas=1 statefulset.apps/prometheus-k8s -n openshift-monitoring || true
    oc scale --replicas=1 statefulset.apps/alertmanager-main -n openshift-monitoring || true
    oc patch etcd cluster -p='{"spec": {"unsupportedConfigOverrides": {"useUnsupportedUnsafeNonHANonProductionUnstableEtcd": true }}}' --type=merge
    oc patch authentications.operator.openshift.io cluster -p='{"spec": {"unsupportedConfigOverrides": {"useUnsupportedUnsafeNonHANonProductionUnstableOAuthServer": true }}}' --type=merge
  args:
    executable: /bin/bash
  register: single_node_cluster
  changed_when: "single_node_cluster.rc == 0"
  delegate_to: "{{ kubeinit_provision_service_node }}"
  when: (groups['all_controller_nodes'] | count) == 1

# This can take a lot of time until the cluster converges
- name: Wait for bootstrap to complete
  ansible.builtin.shell: |
    openshift-install gather --dir install_dir bootstrap --bootstrap {{ hostvars[groups['extra_nodes'][0]].ansible_host }} \
        {% for node in groups['all_controller_nodes'] -%}{% raw %} --master {% endraw %}{{ hostvars[node].ansible_host }}{%- endfor %}

    openshift-install --dir=install_dir/ wait-for bootstrap-complete --log-level info
  args:
    executable: /bin/bash
  register: result
  retries: 5
  delay: 20
  until: result.rc == 0
  changed_when: "result.rc == 0"
  delegate_to: "{{ kubeinit_provision_service_node }}"

- name: Remove bootstrap node from haproxy config
  ansible.builtin.command: |
    podman --remote --connection {{ kubeinit_haproxy_service_node }} exec {{ kubeinit_haproxy_service_name }} sed -i '/bootstrap/s/^/#/' /usr/local/etc/haproxy/haproxy.cfg
  register: remove_bootstrap_from_haproxy
  changed_when: "remove_bootstrap_from_haproxy.rc == 0"
  delegate_to: localhost

- name: Restart haproxy container
  ansible.builtin.systemd:
    name: "{{ kubeinit_haproxy_service_name }}"
    state: restarted
    enabled: yes
    scope: user
  delegate_to: "{{ hostvars[kubeinit_haproxy_service_node].target }}"

# To run in the hypervisor where the bootstrap machine is deployed
- name: Remove boostrap node
  block:

    - name: Destroy bootstrap node VM
      community.libvirt.virt:
        name: "{{ hostvars[kubeinit_deployment_node_name].guest_name }}"
        command: destroy

    - name: Undefine bootstrap node VM
      community.libvirt.virt:
        name: "{{ hostvars[kubeinit_deployment_node_name].guest_name }}"
        command: undefine

    - name: Remove bootstrap node VM storage
      ansible.builtin.file:
        state: absent
        path: "{{ kubeinit_libvirt_target_image_dir }}/{{ hostvars[kubeinit_deployment_node_name].guest_name }}.qcow2"

  vars:
    kubeinit_deployment_node_name: "{{ groups['extra_nodes'][0] }}"
  delegate_to: "{{ hostvars[kubeinit_deployment_node_name].target }}"

- name: Deploy the cluster worker nodes
  ansible.builtin.include_role:
    name: "../../roles/kubeinit_libvirt"
    tasks_from: deploy_coreos_guest.yml
    public: yes
  with_items:
    - "{{ groups['all_compute_nodes'] | list }}"
  loop_control:
    loop_var: cluster_role_item
  vars:
    kubeinit_deployment_node_name: "{{ cluster_role_item }}"
    kubeinit_deployment_delegate: "{{ hostvars[cluster_role_item].target }}"
    kubeinit_deployment_role: worker

- name: Add task-deploy-cluster to tasks_completed
  ansible.builtin.add_host:
    name: "{{ kubeinit_cluster_facts_name }}"
    tasks_completed: "{{ kubeinit_cluster_hostvars.tasks_completed | union(['task-deploy-cluster']) }}"

- name: Update kubeinit_cluster_hostvars
  ansible.builtin.set_fact:
    kubeinit_cluster_hostvars: "{{ hostvars[kubeinit_cluster_facts_name] }}"

- block:
    - name: Check to see if we should stop here
      ansible.builtin.debug: msg="Stopping after '{{ kubeinit_stop_after_task }}'"
    - name: End play
      ansible.builtin.meta: end_play
  when: kubeinit_stop_after_task is defined and kubeinit_stop_after_task in kubeinit_cluster_hostvars.tasks_completed
